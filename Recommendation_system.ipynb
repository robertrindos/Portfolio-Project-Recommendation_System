{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, util\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_critic_reviews_csv(critic_reviews_url):\n",
    "    critic_reviews = pd.read_csv(critic_reviews_url)\n",
    "    return critic_reviews\n",
    "\n",
    "def open_movies_csv(movies_url):\n",
    "    movies = pd.read_csv(movies_url)\n",
    "    return movies\n",
    "movies = open_movies_csv(movies_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_orig_columns_na(movies, critic_reviews):\n",
    "    print(\"critic_reviews.csv NA ccount: \", critic_reviews.isna().sum())\n",
    "    print(\"critic_reviews.csv length: \", len(critic_reviews))\n",
    "    print(\" \")\n",
    "    print(\"movies.csv NA count: \", movies.isna().sum())\n",
    "    print(\"movies.csv length: \", len(movies))\n",
    "#print_orig_columns_na(movies, critic_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_to_space(text_df):\n",
    "    for col in text_df:\n",
    "        text_df[col].apply(lambda x: str(x).replace(',', ' '))\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text_df):\n",
    "    for col in text_df:\n",
    "        text_df[col].apply(lambda x: str(x).replace(' ', ''))\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_movie_dups(df):\n",
    "    df.drop_duplicates('movie_title').reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=drop_movie_dups(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_count_features(df, feature_list):\n",
    "    count_feature_list = ['genres', 'directors', 'authors', 'actors', 'production_company']\n",
    "    count_features = df[count_feature_list]\n",
    "    return count_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_features = define_count_features(movies)\n",
    "#count_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_count_features(count_features):\n",
    "    clean_features = count_features.fillna(' ', inplace=False)\n",
    "    for col in clean_features:\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace(' ', ''))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace(',', ' '))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).lower())\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace('[^\\w\\s]+',''))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace('&', ' '))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace('.', ''))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace('<', ''))\n",
    "        clean_features[col] = clean_features[col].apply(lambda x: str(x).replace('>', ''))\n",
    "    return clean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_features=clean_count_features(count_features)\n",
    "#clean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_bow(clean_features):\n",
    "    count_features = ['genres', 'directors', 'authors', 'actors', 'production_company']\n",
    "    count_bow = clean_features[count_features].agg(' '.join, axis=1)\n",
    "    return count_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_bow = create_count_bow(clean_features)\n",
    "#count_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(bow):\n",
    "    token_array=[]\n",
    "    for text in bow:\n",
    "        tokens=word_tokenize(text)\n",
    "        token_array.append(tokens)\n",
    "    token_df = pd.DataFrame(token_array)\n",
    "    token_df.replace(to_replace=[None], value=' ', inplace=True)\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized=tokenize(count_bow)\n",
    "#tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(count_bow):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit(count_bow)\n",
    "    count_transform = count_vectorizer.transform(count_bow)\n",
    "    soup_count_array = count_transform.toarray()\n",
    "    count_vector = pd.DataFrame(soup_count_array)\n",
    "    return count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-91176d75ea9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcount_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_bow' is not defined"
     ]
    }
   ],
   "source": [
    "count_vector = count_vectorize(count_bow)\n",
    "count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vect_compact(count_vect):\n",
    "    count_vect_compact = soup_count_df.loc[:,(soup_count_df.sum(axis=0) > 1)]\n",
    "    return count_vect_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect_compact=count_vect_compact(soup_count_df)\n",
    "#count_vect_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By only selecting terms (columns) that are included in more than one movie, we reduce a lot of unnecessary computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_tfidf_matrix(df):\n",
    "    tfidf_feature_list = ['movie_info']\n",
    "    tfidf_features = df[tfidf_feature_list]\n",
    "    tfidf_features.fillna(' ', inplace=True, axis=0)\n",
    "    tfidf_matrix = df[tfidf_features]\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_matrix = define_tfidf_matrix(movies)\n",
    "#tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tfidf_matrix(tfidf_matrix):\n",
    "    cleaned_tfidf_matrix = tfidf_matrix.apply(lambda x: remove_punctuation(x))\n",
    "    cleaned_tfidf_matrix = pd.Series(cleaned_tfidf_matrix)\n",
    "    cleaned_tfidf_matrix = lowercase_text(cleaned_tfidf_matrix)\n",
    "    return clean_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_tfidf_matrix = clean_tfidf_matrix(movies['movie_info'])\n",
    "#clean_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_bow(clean_tfidf_matrix):\n",
    "    tfidf_bow = pd.Series([y for x in clean_tfidf_matrix.values.flatten() for y in x.split()]).value_counts()\n",
    "    return tfidf_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_bow = create_tfidf_bow(clean_tfidf_matrix)\n",
    "#tfidf_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(tfidf_bow):\n",
    "    tfidf_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit(tfidf_bow)\n",
    "    tfidf_transform = tfidf_vectorizer.transform(tfidf_bow)\n",
    "    soup_tfidf_array = tfidf_transform.toarray()\n",
    "    soup_tfidf_df = pd.DataFrame(soup_tfidf_array)\n",
    "    return soup_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup_tfidf_df = tfidf_vectorize(tfidf_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommendation(title, cosine_sim):\n",
    "    title=title.replace(' ', '',regex=True).lower()\n",
    "    index=movies_no_dups.index\n",
    "    titles=pd.Series(titles.replace(' ','',regex=True).lower())\n",
    "    indices=pd.Series(index, index=titles)\n",
    "    idx = indices[title]\n",
    "    sim_list = list(enumerate(cosine_sim[idx]))\n",
    "    sim_list = sorted(sim_list, key=lambda x: x[1], reverse=True)\n",
    "    top_10_list = sim_list[1:11]\n",
    "    movie_indices = [i[0] for i in top_10_list]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Features for Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.5 GiB for an array with shape (17712, 223206) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-9f207172eaf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-122-9f207172eaf0>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Creates the count matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mcount_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcompact_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_vect_compact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-1751b35b5632>\u001b[0m in \u001b[0;36mcount_vectorize\u001b[1;34m(count_bow)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcount_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msoup_count_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_transform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcount_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup_count_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcount_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.5 GiB for an array with shape (17712, 223206) and data type int64"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    favorite_movie='Pulp Fiction'\n",
    "    movies_url = 'https://raw.githubusercontent.com/robertrindos/Recommendation-System/main/rotten_tomatoes_movies.csv'\n",
    "    #critic_reviews_url = 'poop'\n",
    "    \n",
    "    # Opens and reads CSV files\n",
    "    movies = open_movies_csv(movies_url)\n",
    "    \n",
    "    \n",
    "    # Defining features and title\n",
    "    count_vec_features = ['genres', 'directors', 'authors', 'actors', 'production_company']\n",
    "    titles = movies['movie_title']\n",
    "    \n",
    "    # Drops duplicate values\n",
    "    movies_no_dups = drop_movie_dups(movies)\n",
    "    \n",
    "    # Defines features for creating word-count matrix\n",
    "    count_features = define_count_features(movies_no_dups, count_vec_features)\n",
    "    \n",
    "    # Text processing/cleaning for count matrix\n",
    "    clean_features = clean_count_features(count_features)\n",
    "    \n",
    "    # Creates a 'Bag of Words' for each movie, this combines all inlcuded terms\n",
    "    count_bow = create_count_bow(clean_features)\n",
    "    \n",
    "    # Creates the count matrix\n",
    "    count_vector = count_vectorize(count_bow)\n",
    "    \n",
    "    compact_vector=count_vect_compact(count_vector)\n",
    "    \n",
    "    cosine_sim = cosine_similarity(compact_vector, compact_vector)\n",
    "    \n",
    "    recommendation_list = movie_recommendation(favorite_movie, cosine_sim)\n",
    "    print(recommendation_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
